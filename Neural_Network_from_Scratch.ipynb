{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSCmIhbM308+zp1UxcjHMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebluong005/Neural-Network-from-Scracth/blob/main/Neural_Network_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA-Cf4tdMF0a"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from random import random\n",
        "import math\n",
        "\n",
        "#function to create initial Neural Network\n",
        "#takes in parameters for inputs, hidden nodes and outputs\n",
        "def init_NN(nodes_in, nodes_h, nodes_out):\n",
        "\n",
        "    #create list for all initalised nodes and their weights\n",
        "    init = []\n",
        "    #create list for hidden nodes\n",
        "    layer_h = []\n",
        "    #create list for output nodes\n",
        "    layer_out = []\n",
        "\n",
        "    #for all nodes in hidden append random weight to dict for each input node +1\n",
        "    #there will be extra input weight for the bias\n",
        "    for i in range(nodes_h):\n",
        "        l = []\n",
        "        for j in range(nodes_in+1):\n",
        "            l.append(random())\n",
        "        dict_weights = {\"w\":l}\n",
        "        layer_h.append(dict_weights)\n",
        "\n",
        "    #for all nodes in output append random weight to dict for each hidden node +1\n",
        "    #there will be extra hidden weight for the bias\n",
        "    for i in range(nodes_out):\n",
        "        l = []\n",
        "        for j in range(nodes_h+1):\n",
        "            l.append(random())\n",
        "        dict_weights = {\"w\":l}\n",
        "        layer_out.append(dict_weights)\n",
        "\n",
        "    #append both hidden and output lists to initalised Neural Network\n",
        "    init.append(layer_h)\n",
        "    init.append(layer_out)\n",
        "    return init\n",
        "\n",
        "\n",
        "#tanh activiate weights function\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "#tanh derivative activate weights function for gradiant descent\n",
        "\n",
        "def grad_tanh(x):\n",
        "    return (1-np.square(x))\n",
        "\n",
        "#function to forward propagate through network\n",
        "\n",
        "def forward_prop(init, inputs):\n",
        "    for layer in init:\n",
        "        #new list to update weights\n",
        "        updated_weights = []\n",
        "\n",
        "        for node in layer:\n",
        "            #get the weights from the dictionary for eachnode\n",
        "            weights = node['w']\n",
        "            #inialised to last weight in dict\n",
        "            initalise = weights[-1]\n",
        "            #add new weights  = old weights  * input data\n",
        "            for i in range(len(weights)-1):\n",
        "                initalise += weights[i] * inputs[i]\n",
        "            #use tanh activation on new weights as non-linear model\n",
        "            node['o'] = tanh(initalise)\n",
        "            #output this as inputs for next layer\n",
        "            updated_weights.append(node['o'])\n",
        "        inputs = updated_weights\n",
        "    return inputs\n",
        "\n",
        "#cost function to determine the error of each node for gradient descent\n",
        "\n",
        "def cost(init, output):\n",
        "    for row in reversed(range(len(init))):\n",
        "        layer = init[row]\n",
        "        cost = []\n",
        "        #for nodes except the output find the cost of that node\n",
        "        if row != len(init)-1:\n",
        "            for cell in range(len(layer)):\n",
        "                #inialise cost to zero\n",
        "                node_cost = 0.0\n",
        "                #get the cost of each node in layer by finding error\n",
        "                #multiply delta value of node with the weight of that node\n",
        "                for node in init[row + 1]:\n",
        "                    node_cost += (node['w'][cell] * node['d'])\n",
        "                cost.append(node_cost)\n",
        "        else:\n",
        "            #for output node cost is predicted output - actual value\n",
        "            for cell in range(len(layer)):\n",
        "                node = layer[cell]\n",
        "                cost.append(output[cell] - node['o'])\n",
        "        for cell in range(len(layer)):\n",
        "            #get the cost for each node node multiplying error by the derivative of activation\n",
        "            #tanh on the output node\n",
        "            #assign this to delta in node which will be used to update weights\n",
        "            node = layer[cell]\n",
        "            node['d'] = cost[cell] * grad_tanh(node['o'])\n",
        "\n",
        "#back propagte through the neural network updating the weights with SGD\n",
        "\n",
        "def back_prop(init, data, learning, epoch, output):\n",
        "    #repeat feed forward and backwards for each epoch\n",
        "    for e in range(epoch):\n",
        "        epoch_error = 0\n",
        "        for inputs in data:\n",
        "            #for every row in training set call forward propagate\n",
        "            result = forward_prop(init, inputs)\n",
        "            #give the label ouputs as parameters for find the cost of nodes\n",
        "            outputs = [0 for out in range(output)]\n",
        "            outputs[inputs[-1]] = 1\n",
        "            cost(init, outputs)\n",
        "            epoch_error += sum([(outputs[out]-result[out])**2 for out in range(len(outputs))])\n",
        "            #update weights based on new cost\n",
        "            for layer in range(len(init)):\n",
        "                update_input = inputs[:-1]\n",
        "                #for all layers add output result to list\n",
        "                if layer !=0:\n",
        "                    update_input = []\n",
        "                    for node in init[layer-1]:\n",
        "                        update_input.append(node['o'])\n",
        "                #for every node in each layer change the weights based on formula SGD\n",
        "                # ( learning rate * cost of error of that node * the output of node )\n",
        "                for node in init[layer]:\n",
        "                    for cell in range(len(update_input)):\n",
        "                        node['w'][cell] += learning*node['d']*update_input[cell]\n",
        "        print(\"epochs: \", e,\" learning rate: \", learning, \"error: \",epoch_error)\n",
        "\n",
        "#method to predict the y values of each row in test returning max from forward propagation of the model\n",
        "\n",
        "def predict(init, row):\n",
        "    out = forward_prop(init, row)\n",
        "    return out.index(max(out))\n",
        "\n",
        "#method to train and test the Neural Network\n",
        "\n",
        "def train_NN(train, test, learning, epoch, hidden):\n",
        "    #set the amount of inputs to NN by the length of columns for parameters\n",
        "    inputs = len(train[0]) - 1\n",
        "    #set amount of outputs to all values outputted ie. 0/1\n",
        "    outputs = len(set([row[-1] for row in train]))\n",
        "    #call init_NN function to set amount of nodes in each layer and their weights\n",
        "    init = init_NN(inputs, hidden,outputs)\n",
        "    #call back propagate function to train model\n",
        "    back_prop(init, train, learning, epoch, outputs)\n",
        "    #create list of predictions for each row of test data\n",
        "    y_pred = []\n",
        "    for row in test:\n",
        "        row_predict = predict(init, row)\n",
        "        y_pred.append(row_predict)\n",
        "    return(y_pred)"
      ],
      "metadata": {
        "id": "Uh1Njy6AMLWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use panads to read csv skipping first line column names\n",
        "df = pd.read_csv('circles500.csv', header = None, skiprows=1)\n",
        "\n",
        "#scale values from mix to max to normalise results\n",
        "scaler = MinMaxScaler()\n",
        "data = pd.DataFrame(scaler.fit_transform(df))\n",
        "\n",
        "#split the data into training and test with 60-40% split most effective\n",
        "train, test = train_test_split(df, test_size=0.34)\n",
        "\n",
        "#set learning rate, number of epochs and number of hidden layers in model\n",
        "learning = 0.1\n",
        "epoch = 1000\n",
        "hidden_nodes = 5\n",
        "\n",
        "#convert the dataframes to list\n",
        "train =train.values.tolist()\n",
        "test = test.values.tolist()\n",
        "\n",
        "#for the training and test set convert the label of each row to an integer from a float\n",
        "for x in range(len(train)):\n",
        "    train[x][-1] = int(train[x][-1])\n",
        "\n",
        "for y in range(len(test)):\n",
        "    test[y][-1] = int(test[y][-1])\n",
        "\n",
        "#get the actual output label from the test set\n",
        "y_test = []\n",
        "for row in range(len(test)):\n",
        "    y_test.append(test[row][-1])\n",
        "\n",
        "#remove the label for test set so can relabel with prediction\n",
        "for row in range(len(test)):\n",
        "    #assign label to None\n",
        "    test[row][-1] = None\n",
        "#get the predicted labels\n",
        "y_pred = train_NN(train, test, learning, epoch, hidden_nodes)\n",
        "\n",
        "#get the accuracy of the model\n",
        "pred_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Prediction Accuracy = \", pred_acc)"
      ],
      "metadata": {
        "id": "AbwzM8QIMXzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function taken from the CIFAR website\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "data_dict = unpickle(\"data_batch_1\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
        "\n",
        "data1 = data_dict[b'data']\n",
        "\n",
        "\"\"\" This function converts the raw array of 3072 floats describing each image into a structure where the colour of each pixel\n",
        "in an image is represented by its r,g,b (in this order) value for every pixel in the image in question.\n",
        "The input into the function is an array of 3072 floats.\n",
        "This assumes that the raw data coming from the CIFAR data is in the format where all the r values come first, followed by\n",
        "all the b values and then all the g values \"\"\"\n",
        "def convert_rgb(dataplusindex):\n",
        "    rgb_picture = dataplusindex\n",
        "    rgb_picture.shape = (3,32,32)\n",
        "    rgb_picture = rgb_picture.transpose([1, 2, 0])\n",
        "\n",
        "    return rgb_picture\n",
        "\n",
        "\n",
        "\"\"\"This function converts the R,G,B pixels into greyscale pixels. There are a few methods for doing this.\n",
        "If the data is ordered correctly as R,G,B it is appropriate to use Method 2 (link below) which multiplies each component colour by\n",
        "a certain constant, and then sums the total.\n",
        "If not, one can use a standard average of the R,G,B, Method 1 (link below)\"\"\"\n",
        "def convert_greyscale(rgb_pic):\n",
        "    greyscale_array = []\n",
        "    for element in rgb_pic:\n",
        "        for rgb_row in element:\n",
        "            x = round(np.dot(rgb_row,[0.299, 0.587, 0.114]), 2) #https://www.prasannakumarr.in/journal/color-to-grayscale-python-image-processing. Method 2, assuming the rgb values come in in the format r, g, b and not b,g,r for example\n",
        "            #x = (rgb_row[0] + rgb_row[1] + rgb_row[2])/3 # https://www.prasannakumarr.in/journal/color-to-grayscale-python-image-processing. Method 1\n",
        "            greyscale_array.append(x)\n",
        "    greyscale_array = np.array(greyscale_array)\n",
        "    return greyscale_array\n",
        "\n",
        "# Converting the data into format [greyscale list, classification].\n",
        "# Each data instance will be added to a larger list.\n",
        "\n",
        "#Frog is class 6 in the CIFAR data\n",
        "#Deer is class 4 in the CIFAR data\n",
        "\n",
        "# Choosing Frog to be class 1 for our implementation\n",
        "# Choosing Deer to be class 0 for our implementation\n",
        "\n",
        "Frog_Deer_classified_list = []\n",
        "for i in range(len(data_dict[b'labels'])):\n",
        "    if data_dict[b'labels'][i] == 4 or data_dict[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
        "        rgb_pic = convert_rgb(data_dict[b'data'][i])\n",
        "        greyscale_arr = convert_greyscale(rgb_pic)\n",
        "        if data_dict[b'labels'][i] == 4:\n",
        "            greyscale_arr = np.append(greyscale_arr,0)\n",
        "            Frog_Deer_classified_list.append(greyscale_arr)\n",
        "\n",
        "        if data_dict[b'labels'][i] == 6:\n",
        "            greyscale_arr = np.append(greyscale_arr,1)\n",
        "            Frog_Deer_classified_list.append(greyscale_arr)\n",
        "\n",
        "\n",
        "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
        "new_Frog_Deer_classified_list = []\n",
        "for element in Frog_Deer_classified_list:\n",
        "    x = element.tolist()\n",
        "    new_Frog_Deer_classified_list.append(x)\n",
        "\n",
        "# Creating Pandas Data Frame from the filtered data\n",
        "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
        "df_train = pd.DataFrame(new_Frog_Deer_classified_list)\n",
        "#print(df_train)\n",
        "\n",
        "\n",
        "\n",
        "data_dict_test = unpickle(\"test_batch\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
        "\n",
        "data_test = data_dict_test[b'data']\n",
        "\n",
        "# Converting the data into format [greyscale list, classification].\n",
        "# Each data instance will be added to a larger list.\n",
        "\n",
        "#Frog is class 6 in the CIFAR data\n",
        "#Deer is class 4 in the CIFAR data\n",
        "\n",
        "# Choosing Frog to be class 1 for our implementation\n",
        "# Choosing Deer to be class 0 for our implementation\n",
        "\n",
        "Frog_Deer_classified_list_test = []\n",
        "for i in range(len(data_dict_test[b'labels'])):\n",
        "    if data_dict_test[b'labels'][i] == 4 or data_dict_test[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
        "        rgb_pic_test = convert_rgb(data_dict[b'data'][i])\n",
        "        greyscale_arr_test = convert_greyscale(rgb_pic_test)\n",
        "        if data_dict_test[b'labels'][i] == 4:\n",
        "            greyscale_arr_test = np.append(greyscale_arr_test,0)\n",
        "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
        "\n",
        "        if data_dict_test[b'labels'][i] == 6:\n",
        "            greyscale_arr_test = np.append(greyscale_arr_test,1)\n",
        "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
        "\n",
        "\n",
        "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
        "new_Frog_Deer_classified_list_test = []\n",
        "for element in Frog_Deer_classified_list_test:\n",
        "    x = element.tolist()\n",
        "    new_Frog_Deer_classified_list_test.append(x)\n",
        "\n",
        "# Creating Pandas Data Frame from the filtered data\n",
        "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
        "df_test = pd.DataFrame(new_Frog_Deer_classified_list_test)\n",
        "df_test"
      ],
      "metadata": {
        "id": "A_VkHJJhMdDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set learning rate, number of epochs and number of hidden layers in model\n",
        "learning = 0.1\n",
        "epoch = 20\n",
        "hidden_nodes = 12\n",
        "\n",
        "#convert the dataframes to list\n",
        "train =df_train.values.tolist()\n",
        "test = df_test.values.tolist()\n",
        "\n",
        "#for the training and test set convert the label of each row to an integer from a float\n",
        "for x in range(len(train)):\n",
        "    train[x][-1] = int(train[x][-1])\n",
        "\n",
        "for y in range(len(test)):\n",
        "    test[y][-1] = int(test[y][-1])\n",
        "\n",
        "#get the actual output label from the test set\n",
        "y_test = []\n",
        "for row in range(len(test)):\n",
        "    y_test.append(test[row][-1])\n",
        "\n",
        "#remove the label for test set so can relabel with prediction\n",
        "for row in range(len(test)):\n",
        "    #assign label to None\n",
        "    test[row][-1] = None\n",
        "#get the predicted labels\n",
        "y_pred1 = train_NN(train, test, learning, epoch, hidden_nodes)\n",
        "\n",
        "#get the accuracy of the model\n",
        "x_new = accuracy_score(y_test, y_pred1)\n",
        "print(\"Prediction accuracy of model\", x_new)"
      ],
      "metadata": {
        "id": "w0w13ngPMkwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kN8b8hq-MnA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}